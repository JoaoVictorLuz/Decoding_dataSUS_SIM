# Projeto Final do Bootcamp da Triggo.ai

## üöÄ Sobre o Projeto
Este √© o **projeto final do Bootcamp da Triggo.ai**, desenvolvido para aplicar conhecimentos de **Python, SQL, ETL, Data Analysis e boas pr√°ticas de desenvolvimento de software**.  
O objetivo √© construir um **pipeline de dados completo**.

[Clique aqui para acessar o pitch](https://youtu.be/XZ9DrtrXVCQ)
---

![Arquitetura do projeto](readme_assets/mapa.jpg)

## Observa√ß√£o
Por seguran√ßa minhas credenciais do snowflake n√£o ser√£o disponibilizadas, mas deixo aqui um preview de como os dados se parecem quando o pipeline funciona: 


![img2](readme_assets/data_lake_stg.png)
![img3](readme_assets/data_lake_tables.png)
![img1](readme_assets/data_warehouse_views.png)
![img4](readme_assets/data_warehouse_final_table.png)

## Infelizmente ainda n√£o √© poss√≠vel rodar o projeto üò•üò•üò•
Mas n√£o se preocupe, em breve farei modifica√ß√µes que permitam que qualquer um com uma conta no snowflake possa rodar esse pipeline.

Mas por que n√£o √© poss√≠vel executar o projeto ü§∑‚Äç‚ôÇÔ∏èü§∑‚Äç‚ôÇÔ∏èü§∑‚Äç‚ôÇÔ∏è? 
1. Ele n√£o foi feito com o objetivo de ser executado para o p√∫blico geral!!! Isso faz parte do desafio do Bootcamp da Triggo.ai portanto essa √© uma resolu√ß√£o de um desafio, n√£o um pipeline de c√≥digo aberto.
2. Esta n√£o √© a vers√£o enviada como resolu√ß√£o do desafio!!! A vers√£o enviada est√° em outro reposit√≥rio privado, o c√≥digo desse reposit√≥rio foi intensamente modificado para evitar a utiliza√ß√£o das minhas ‚ùå**credenciais do snowflake**‚ùå.
3. Este trabalho ainda n√£o chegou na sua vers√£o final!!! Ainda faltam muitas modifica√ß√µes para deixar o c√≥digo mais leg√≠vel, organizado, modularizado, orquestrado, conteinerizado e de f√°cil execu√ß√£o.



## Se fosse poss√≠vel rodar o projeto seria algo do tipo:
- Na pasta home do seu computador um diretorio .dbt deve ser criado e dentro dessa pasta crie um "profiles.yml"
- As dependencias disponiveis dentro de "environment.yml" devem ser instaladas dentro de um virtual environment (nesse projeto o conda foi utilizado)
- Ao instalar as dependencias relacionadas ao dbt, modifique e depopis copie e cole o texto de "profiles.txt" do "profiles.yml" no .dbt
- No terminal rode o comando "dbt debug" para testar a conex√£o 
- Utilize como base o .env.example e crie um .evn com as suas credenciais do snowflake
- Com todo o ambiente configurado execute os scripts python na seguinte ordem
    1 - datasus_to_local.py
    2 - parquet_to_csv.py
    Antes de seguir os pr√≥ximos passo apague todas as tabelas e views do database do snowflake "DATASUS" para evitar a duplica√ß√£o dos arquivos
    3 - local_to_snowflake.py
    4 - snowflake_creating_tables.py
    5 - snowflake_stg_to_db.py
- Assim rode no terminal "dbt run" para trasformar os dados e envia-los para o data warehouse
